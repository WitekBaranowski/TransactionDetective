{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to pull your data from Feature Store for training, run data exploratory analysis on your data, build a machine learning model locally, experiment with various hyperparameters, evaluate the model and deloy it to a Vertex AI endpoint. \n",
    "\n",
    "### Objective\n",
    "\n",
    "In the following notebook, you will learn how to:\n",
    "\n",
    "* Use a Feature Store to pull training data\n",
    "* Do some exploratory analysis on the extracted data\n",
    "* Train the model and track the results using Vertex Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an experiment name\n",
    "\n",
    "Let us define the experiment name to store . If EXEPERIMENT_NAME is not set, set a default one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIOrI-hoJ46P"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"\"  # @param {type:\"string\"}\n",
    "if EXPERIMENT_NAME == \"\" or EXPERIMENT_NAME is None:\n",
    "    EXPERIMENT_NAME = \"fd-experiment-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "import random\n",
    "\n",
    "#Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Model Training\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1 import ModelServiceClient\n",
    "from google.cloud.aiplatform_v1.types import ListModelEvaluationsRequest\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud import storage\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import json\n",
    "import logging\n",
    "import xgboost as xgb\n",
    "\n",
    "#Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import Featurestore, EntityType, Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "DATA_DIR = os.path.join(os.pardir, 'data')\n",
    "TRAIN_DATA_DIR = os.path.join(DATA_DIR, 'train')\n",
    "DATA_URI = f'gs://{BUCKET_NAME}/data'\n",
    "TRAIN_DATA_URI = f'{DATA_URI}/train'\n",
    "CONFIG_URI =  f'gs://{BUCKET_NAME}/config' \n",
    "BQ_DATASET = \"tx\"\n",
    "\n",
    "#Feature Store\n",
    "START_DATE_TRAIN = \"2022-01-01\" #consider few days for training (demo)\n",
    "END_DATE_TRAIN = \"2022-01-31\"\n",
    "EVENTS_TABLE_NAME = f'events_{END_DATE_TRAIN}'\n",
    "CUSTOMERS_TABLE_NAME = f'customers_{END_DATE_TRAIN}'\n",
    "TERMINALS_TABLE_NAME = f'terminals_{END_DATE_TRAIN}'\n",
    "DATA_ENDPOINT = f\"{REGION}-featurestore-aiplatform.googleapis.com\"\n",
    "ADMIN_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "EVENT_ENTITY = 'event'\n",
    "CUSTOMER_ENTITY = 'customer'\n",
    "TERMINAL_ENTITY = 'terminal'\n",
    "SERVING_FEATURE_IDS = {CUSTOMER_ENTITY: ['*'], EVENT_ENTITY: ['*'], TERMINAL_ENTITY: ['*']}\n",
    "\n",
    "# Training\n",
    "COLUMNS_IGNORE = ['terminal_id', 'customer_id', 'entity_type_event', 'entity_type_customer', 'entity_type_terminal']\n",
    "TARGET = 'tx_fraud'\n",
    "\n",
    "## Custom Training\n",
    "DATASET_NAME=f\"sample_train-{ID}-{END_DATE_TRAIN}\"\n",
    "TRAIN_JOB_NAME=f\"fraudfinder_xgb_train_expr-{ID}-{TIMESTAMP}\"\n",
    "MODEL_NAME=f\"fraudfinder_xgb_model_expr-{ID}-{TIMESTAMP}\"\n",
    "DEPLOYED_NAME = f\"fraudfinder_xgb_prediction_expr-{ID}-{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Inizialize clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME, experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcs_dataset(client,\n",
    "                       display_name: str, \n",
    "                       gcs_source: Union[str, List[str]]):\n",
    "    \n",
    "    dataset = client.TabularDataset.create(\n",
    "        display_name=display_name, gcs_source=gcs_source,\n",
    "    )\n",
    "\n",
    "    dataset.wait()\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "def get_evaluation_metrics(client, model_resource_name):\n",
    "    model_evalution_request = ListModelEvaluationsRequest(parent=model_resource_name)\n",
    "    model_evaluation_list = client.list_model_evaluations(request=model_evalution_request)\n",
    "    metrics_strlist = []\n",
    "    for evaluation in model_evaluation_list:\n",
    "        metrics = MessageToDict(evaluation._pb.metrics)\n",
    "    return metrics\n",
    "\n",
    "def gcs_list(gcs_uri):\n",
    "    obj_list=[]\n",
    "    storage_client = storage.Client()\n",
    "    bucket, key = gcs_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    for blob in storage_client.list_blobs(bucket, prefix=key):\n",
    "        obj_list.append(\"gs://\"+bucket+\"/\"+str(blob.name))\n",
    "    return obj_list\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    dummy_columns = list(df.dtypes[df.dtypes == 'category'].index)\n",
    "    df = pd.get_dummies(df, columns=dummy_columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching feature values for model training\n",
    "\n",
    "To fetch training data, we have to specify the following inputs to batch serving:\n",
    "\n",
    "- A file containing a \"query\", with the entities and timestamps for each label.\n",
    "- List of features to fetch values for\n",
    "- Destination location and format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-instance list\n",
    "\n",
    "In our case, we need a csv file with content formatted like the table below\n",
    "\n",
    "|event|customer                     |terminal|timestamp                                    |\n",
    "|-----|-----------------------------|--------|---------------------------------------------|\n",
    "|xxx57538|xxx3859                         |xxx8811    |2021-07-07 00:01:10 UTC                      |\n",
    "|xxx57539|xxx4165                         |xxx8810    |2021-07-07 00:01:55 UTC                      |\n",
    "|xxx57540|xxx2289                         |xxx2081    |2021-07-07 00:02:12 UTC                      |\n",
    "|xxx57541|xxx3227                         |xxx3011    |2021-07-07 00:03:23 UTC                      |\n",
    "|xxx57542|xxx2819                         |xxx6263    |2021-07-07 00:05:30 UTC                      |\n",
    "\n",
    "where column names are the name of entities in Feature Store and the timestamp represents the time of occured event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"\"\"\n",
    "SELECT\n",
    "  e.TX_ID AS event,\n",
    "  e.CUSTOMER_ID AS customer,\n",
    "  e.TERMINAL_ID AS terminal,\n",
    "  e.TX_TS AS timestamp\n",
    "FROM\n",
    "  `{BQ_DATASET}.{EVENTS_TABLE_NAME}` AS e\n",
    "WHERE\n",
    "  e.TX_TS BETWEEN \"{START_DATE_TRAIN}\" AND \"{END_DATE_TRAIN}\"\n",
    "LIMIT {TRAINING_DS_SIZE}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    query_df = bq_client.query(sql_query).to_dataframe()\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "query_df['timestamp'] = pd.to_datetime(query_df.timestamp).dt.tz_convert('UTC').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "query_df['timestamp'] = pd.to_datetime(query_df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ff_feature_store = Featurestore(FEATURESTORE_ID)\n",
    "except NameError:\n",
    "    print(f\"\"\"The feature store {FEATURESTORE_ID} does not exist!\"\"\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a training sample of data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = ff_feature_store.batch_serve_to_df(\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_df=query_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "Here we use a subset of data for data exploration and better underestanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('fraud_reported distribution')\n",
    "sns.countplot(x='tx_fraud', data=sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sample_df.tx_fraud.value_counts()/sample_df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data is imbalanced. We will fix this later in this notebook before building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sample_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ 'tx_amount', 'customer_id_nb_tx_7day_window', 'customer_id_nb_tx_14day_window',\n",
    "             'customer_id_nb_tx_1day_window', 'customer_id_avg_amount_7day_window', 'customer_id_avg_amount_14day_window',\n",
    "             'customer_id_avg_amount_1day_window', 'terminal_id_risk_7day_window', 'terminal_id_risk_14day_window',\n",
    "             'terminal_id_risk_1day_window', 'terminal_id_nb_tx_14day_window', 'terminal_id_nb_tx_1day_window',\n",
    "            'terminal_id_nb_tx_7day_window']\n",
    "\n",
    "sample_df[features].hist(figsize = (20, 10), grid=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the relationship between features and ```tx_fraud```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(sample_df[sample_df.columns.difference(COLUMNS_IGNORE)].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "Based on the EDA of the sample:\n",
    "\n",
    "- the sample is super unbalanced\n",
    "- we need to delete some variables\n",
    "- we could convert timestamp and extract variables such as day week, night, etc (calculate embeddings)\n",
    "- we could do variable selection\n",
    "- we need to scale variables\n",
    "\n",
    "## Builing a custom fraud detection model\n",
    "\n",
    "### Fixing Imbalanced Dataset\n",
    "In this section, we will deal with imbalance dataset. Specifically, we will randomly delete some of the instances from the non-fraud class in order to match the numbers with the fraud class. This technique is called undersampling.For this workshop, we will skip the data balance process, as our sample data is very samll. Please uncomment the following cell if you have collected a big sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_df = sample_df.sample(frac=1,random_state=4)\n",
    "# fraud_df = shuffled_df.loc[shuffled_df['tx_fraud'] == 1]\n",
    "# non_fraud_df = shuffled_df.loc[shuffled_df['tx_fraud'] == 0].sample(n=fraud_df.shape[0],random_state=42)\n",
    "# balanced_df = pd.concat([fraud_df, non_fraud_df])\n",
    "# (balanced_df.tx_fraud.value_counts()/balanced_df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this section, we will use xgboost algorithm. Typically, to perform training, you will use Vertex AI traning pipeline, however, as we are experimenting, we simply use xgboost package to tranin our model in the notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing datasets\n",
    "In the follwing cell, we split data into training, test, and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training variables\n",
    "LABEL_COLUMN = \"tx_fraud\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_event\",\"terminal_id\",\"customer_id\",\"entity_type_customer\",\"entity_type_terminal\"]\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "df_dataset = sample_df\n",
    "df_train, df_test, df_val = np.split(df_dataset.sample(frac=1, random_state=42), [int(.6*len(df_dataset)), int(.8*len(df_dataset))])\n",
    "\n",
    "#training set\n",
    "preprocessed_train_Data = preprocess(df_dataset)\n",
    "x_train = preprocessed_train_Data[preprocessed_train_Data.columns.drop(LABEL_COLUMN).to_list()].values\n",
    "y_train = preprocessed_train_Data.loc[:,LABEL_COLUMN].astype(int)\n",
    "\n",
    "#validation set\n",
    "preprocessed_val_Data = preprocess(df_val)\n",
    "x_val = preprocessed_val_Data[preprocessed_val_Data.columns.drop(LABEL_COLUMN).to_list()].values\n",
    "y_val = preprocessed_val_Data.loc[:,LABEL_COLUMN].astype(int)\n",
    "\n",
    "#test set\n",
    "preprocessed_test_Data = preprocess(df_test)\n",
    "x_test = preprocessed_test_Data[preprocessed_test_Data.columns.drop(LABEL_COLUMN).to_list()].values\n",
    "y_test = preprocessed_test_Data.loc[:,LABEL_COLUMN].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "Before running XGBoost, we can set some hyperparameters. It is one of the key to XGBoost performance. As the best practice, you can use Vertex AI HyperParameter Tuning to automatically find the best parameters. However, in this notebook, for the sake of simplicity, we just specify some of these hyperparemeters manually and randomly to experiment.  \n",
    "- eta is a regularization parameter to reduce feature weights in each boosting step. \n",
    "- gamma is another regularization parameter for tree pruning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"eta\": 0.2, \"gamma\": 0.0, \"max_depth\": 4},\n",
    "    {\"eta\": 0.2, \"gamma\": 0.0, \"max_depth\": 5},\n",
    "    {\"eta\": 0.2, \"gamma\": 0.1, \"max_depth\": 4},\n",
    "    {\"eta\": 0.2, \"gamma\": 0.1, \"max_depth\": 5},\n",
    "    {\"eta\": 0.3, \"gamma\": 0.0, \"max_depth\": 4},\n",
    "    {\"eta\": 0.3, \"gamma\": 0.0, \"max_depth\": 5},\n",
    "    {\"eta\": 0.3, \"gamma\": 0.1, \"max_depth\": 4},\n",
    "    {\"eta\": 0.3, \"gamma\": 0.1, \"max_depth\": 5},\n",
    "]\n",
    "models={}\n",
    "for i, params in enumerate(parameters):\n",
    "    run_name=f\"fd-xgboost-local-run-{i}\"\n",
    "    print(run_name)\n",
    "    vertex_ai.start_run(run=run_name)\n",
    "    vertex_ai.log_params(params)\n",
    "    model =  xgb.XGBClassifier( objective='reg:logistic', max_depth = params[\"max_depth\"], gamma = params[\"gamma\"], eta = params[\"eta\"], use_label_encoder=False)\n",
    "    model.fit(x_train, y_train)\n",
    "    models[run_name] = model\n",
    "    y_pred_proba = model.predict_proba(x_val)[:, 1]\n",
    "    y_pred = model.predict(x_val)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    val_f1_score = f1_score(y_val, y_pred, average='weighted')\n",
    "    vertex_ai.log_metrics({\"acc_score\": acc_score, \"f1score\": val_f1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1PqKxlpOZa2"
   },
   "source": [
    "We can also extract all parameters and metrics associated with any Experiment into a dataframe for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbRf1WoH_vbY"
   },
   "outputs": [],
   "source": [
    "experiment_df = vertex_ai.get_experiment_df()\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F19_5lw0MqXv"
   },
   "source": [
    "Also we can visualize experiments in Cloud Console. Run the following to get the URL of Vertex AI Experiments for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmN9vE9pqqzt"
   },
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/ai/platform/experiments/experiments?folder=&organizationId=&project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model\n",
    "Now that we have run our experiments, lets choose one the runs, and use xgboost.Booster's save_model method to export the model to a file named model.bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = '../model'\n",
    "# if not os.path.exists(model_directory):\n",
    "#     os.makedirs(model_directory)\n",
    "\n",
    "!sudo mkdir -p -m 777 {model_directory}\n",
    "\n",
    "model = models['fd-xgboost-local-run-3']\n",
    "artifact_filename = 'model.bst'\n",
    "model_path = os.path.join(model_directory, artifact_filename)\n",
    "model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Lets first test the model locally with the test set, to get predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster()  # init model\n",
    "bst.load_model(model_path) \n",
    "xgtest = xgb.DMatrix(x_test)\n",
    "y_pred_prob = bst.predict(xgtest)\n",
    "y_pred = y_pred_prob.round().astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test.values, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision for converting a predicted probability into a fraudulant/none-fradulant class label is determined by a discrimination threshold, which it default value is 0.5. That is, a transaction is predicted as none-fraudulant (class 0) if the probability is under 0.5, and fraudulant (class 1) if it is equal or over 0.5. In other word, this threshold determines the True Positive, False Pasitive, True Negative and False Negative numbers which are typically used in confusion matrix, precision, recall and F1-score (to be used as accuracy metric of a classification model.) You might get different TP and FP rate if you change this threshold, specially if your data is imbalanced. And, by sliding/moving this threshold, you might find a preferred/optimized spot that leads to optimized model performance based on your business tolerance for accepting cost of FP cases. In the following cell we calculate the confusion matrix for differnt thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,5, figsize=(15,4))\n",
    "     \n",
    "for n, ax in enumerate(axes.flat):\n",
    "    threshold =  (n+1)/10\n",
    "    y_pred = (y_pred_prob > threshold).astype(int)\n",
    "    cfm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cfm, annot=True, cmap=\"Reds\", fmt=\"d\", ax=ax, cbar=False)\n",
    "    ax.title.set_text(\"Threshold=%.2f\" % threshold)\n",
    "  \n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.8)\n",
    "plt.suptitle(\"Confusion Matrix for various thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might get better insigh on the optimal threshold by generation ROC Curve. It will plot the true positive (TP) vs. false positive (FP) rates at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve. Also, we calculate AUC which ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(y_test.values, y_pred, average='weighted')\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test.values, y_pred_prob)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "  \n",
    "plt.figure(figsize=(8, 6))\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.',label=\"xgboost: AUC = %.2f\" % auc)\n",
    "\n",
    "\n",
    "# generate general prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_fpr, ns_tpr, _ = metrics.roc_curve(y_test, ns_probs)\n",
    "ns_auc = metrics.auc(ns_fpr, ns_tpr)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label=\"No Skill: AUC = %.2f\" % ns_auc)\n",
    "\n",
    "  \n",
    "plt.ylabel(\"TP rate\")\n",
    "plt.xlabel(\"FP rate\")\n",
    "  \n",
    "plt.legend(loc=4)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the optimal threshold? As mentioned depends on the business use-case, for example, if we consider the optimal threshold for our usecase is the one that has\n",
    "highest TPR and minimum FPR, then we can calculate it as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal threshold value is: %.3f\" % thresholds[np.argmax(tpr - fpr)])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
