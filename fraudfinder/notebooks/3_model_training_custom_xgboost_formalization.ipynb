{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to get your training dataset from Vertex AI Feature Store, train your model using Vertex AI managed training pipeline, and deployment it into Vertex AI endpoint. You will learn how to use your own custom code for ML training on Vertex AI.\n",
    "\n",
    "### Objective\n",
    "\n",
    "In the following notebook, you will learn how to:\n",
    "\n",
    "    * Build a containers to run your own custom code on Vertex AI\n",
    "    * Use Vertex AI to train your model at scale\n",
    "    * Use Vertex AI to create an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUCKET_NAME          = \"transaction-detective22-1559\"\n",
      "PROJECT              = \"transaction-detective22-1559\"\n",
      "REGION               = \"us-central1\"\n",
      "SUFFIX               = \"aiplatform.googleapis.com\"\n",
      "API_ENDPOINT         = \"us-central1-aiplatform.googleapis.com\"\n",
      "PREDICT_API_ENDPOINT = \"us-central1-prediction-aiplatform.googleapis.com\"\n",
      "FS_NAME              = \"transaction-detective22-1559\"\n",
      "ID                   = \"7551\"\n",
      "FEATURESTORE_ID      = \"fraud_finder_7551\"\n",
      "TIMESTAMP            = \"1647254678\"\n",
      "TRAINING_DS_SIZE     = \"1000\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an experiment name\n",
    "\n",
    "Let us define the experiment name to store . If EXEPERIMENT_NAME is not set, set a default one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"\"  # @param {type:\"string\"}\n",
    "if EXPERIMENT_NAME == \"\" or EXPERIMENT_NAME is None:\n",
    "    EXPERIMENT_NAME = \"fd-experiment-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "\n",
    "#Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Model Training with Vertex AI\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1 import ModelServiceClient\n",
    "from google.cloud.aiplatform_v1.types import ListModelEvaluationsRequest\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "#Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import Featurestore, EntityType, Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "DATA_DIR = os.path.join(os.pardir, 'data')\n",
    "TRAIN_DATA_DIR = os.path.join(DATA_DIR, 'train')\n",
    "# CONFIG_DIR = os.path.join(os.pardir, 'config')\n",
    "DATA_URI = f'gs://{BUCKET_NAME}/data'\n",
    "TRAIN_DATA_URI = f'{DATA_URI}/train'\n",
    "CONFIG_URI =  f'gs://{BUCKET_NAME}/config' \n",
    "BQ_DATASET = \"tx\"\n",
    "\n",
    "#Feature Store\n",
    "START_DATE_TRAIN = \"2022-01-01\" #consider few days for training (demo)\n",
    "END_DATE_TRAIN = \"2022-01-31\"\n",
    "EVENTS_TABLE_NAME = f'events_{END_DATE_TRAIN}'\n",
    "CUSTOMERS_TABLE_NAME = f'customers_{END_DATE_TRAIN}'\n",
    "TERMINALS_TABLE_NAME = f'terminals_{END_DATE_TRAIN}'\n",
    "DATA_ENDPOINT = f\"{REGION}-featurestore-aiplatform.googleapis.com\"\n",
    "ADMIN_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "EVENT_ENTITY = 'event'\n",
    "CUSTOMER_ENTITY = 'customer'\n",
    "TERMINAL_ENTITY = 'terminal'\n",
    "SERVING_FEATURE_IDS = {CUSTOMER_ENTITY: ['*'], EVENT_ENTITY: ['*'], TERMINAL_ENTITY: ['*']}\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{TIMESTAMP}\"\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Training\n",
    "COLUMNS_IGNORE = ['terminal_id', 'customer_id', 'entity_type_event', 'entity_type_customer', 'entity_type_terminal']\n",
    "TARGET = 'tx_fraud'\n",
    "\n",
    "## Custom Training\n",
    "DATASET_NAME=f\"sample_train-{ID}-{END_DATE_TRAIN}\"\n",
    "TRAIN_JOB_NAME=f\"fraudfinder_xgb_train_frmlz-{ID}-{TIMESTAMP}\"\n",
    "MODEL_NAME=f\"fraudfinder_xgb_model_frmlz-{ID}-{TIMESTAMP}\"\n",
    "DEPLOYED_NAME = f\"fraudfinder_xgb_prediction_frmlz-{ID}-{TIMESTAMP}\"\n",
    "MODEL_SERVING_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest'\n",
    "IMAGE_REPOSITORY = f'fraudfinder-{ID}'\n",
    "IMAGE_NAME='dask-xgb-classificator'\n",
    "IMAGE_TAG='v1'\n",
    "IMAGE_URI=f\"us-central1-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "TRAIN_COMPUTE='e2-standard-4'\n",
    "DEPLOY_COMPUTE='n1-standard-4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Inizialize clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME, experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcs_dataset(client,\n",
    "                       display_name: str, \n",
    "                       gcs_source: Union[str, List[str]]):\n",
    "    \n",
    "    dataset = client.TabularDataset.create(\n",
    "        display_name=display_name, gcs_source=gcs_source,\n",
    "    )\n",
    "\n",
    "    dataset.wait()\n",
    "    return dataset\n",
    "    \n",
    "def get_evaluation_metrics(client, model_resource_name):\n",
    "    model_evalution_request = ListModelEvaluationsRequest(parent=model_resource_name)\n",
    "    model_evaluation_list = client.list_model_evaluations(request=model_evalution_request)\n",
    "    metrics_strlist = []\n",
    "    for evaluation in model_evaluation_list:\n",
    "        metrics = MessageToDict(evaluation._pb.metrics)\n",
    "    return metrics\n",
    "\n",
    "def gcs_list(gcs_uri):\n",
    "    obj_list=[]\n",
    "    storage_client = storage.Client()\n",
    "    bucket, key = gcs_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "    for blob in storage_client.list_blobs(bucket, prefix=key):\n",
    "        obj_list.append(\"gs://\"+bucket+\"/\"+str(blob.name))\n",
    "    return obj_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetching feature values for model training\n",
    "\n",
    "To fetch training data, we have to specify the following inputs to batch serving:\n",
    "\n",
    "- A file containing a \"query\", with the entities and timestamps for each label.\n",
    "- List of features to fetch values for\n",
    "- Destination location and format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-instance list\n",
    "\n",
    "In our case, we need a csv file with content formatted like the table below\n",
    "\n",
    "|event|customer                     |terminal|timestamp                                    |\n",
    "|-----|-----------------------------|--------|---------------------------------------------|\n",
    "|xxx57538|xxx3859                         |xxx8811    |2021-07-07 00:01:10 UTC                      |\n",
    "|xxx57539|xxx4165                         |xxx8810    |2021-07-07 00:01:55 UTC                      |\n",
    "|xxx57540|xxx2289                         |xxx2081    |2021-07-07 00:02:12 UTC                      |\n",
    "|xxx57541|xxx3227                         |xxx3011    |2021-07-07 00:03:23 UTC                      |\n",
    "|xxx57542|xxx2819                         |xxx6263    |2021-07-07 00:05:30 UTC                      |\n",
    "\n",
    "where column names are the name of entities in Feature Store and the timestamp represents the time of occured event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}` AS\n",
    "SELECT\n",
    "  e.TX_ID AS event,\n",
    "  e.CUSTOMER_ID AS customer,\n",
    "  e.TERMINAL_ID AS terminal,\n",
    "  e.TX_TS AS timestamp\n",
    "FROM\n",
    "  `{BQ_DATASET}.{EVENTS_TABLE_NAME}` AS e\n",
    "WHERE\n",
    "  e.TX_TS BETWEEN \"{START_DATE_TRAIN}\" AND \"{END_DATE_TRAIN}\"\n",
    "LIMIT {TRAINING_DS_SIZE}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    job = bq_client.query(sql_query)\n",
    "    _ = job.result()\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ff_feature_store = Featurestore(FEATURESTORE_ID)\n",
    "except NameError:\n",
    "    print(f\"\"\"The feature store {FEATURESTORE_ID} does not exist!\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a sample of data to a bucket \n",
    "In this section, we will use Batch Serving of feature store, to prepare dataset for training examples by calling the BatchReadFeatureValues API. Batch Serving is used to fetch a large batch of feature values for high-throughput, typically for training a model or batch prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Uniform bucket-level access for gs://transaction-detective22-1559...\n"
     ]
    }
   ],
   "source": [
    "!gsutil uniformbucketlevelaccess set on gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.featurestore.featurestore:Serving Featurestore feature values: projects/130114571864/locations/us-central1/featurestores/fraud_finder_7551\n",
      "INFO:google.cloud.aiplatform.featurestore.featurestore:Serve Featurestore feature values backing LRO: projects/130114571864/locations/us-central1/featurestores/fraud_finder_7551/operations/5029479065225003008\n",
      "INFO:google.cloud.aiplatform.featurestore.featurestore:Featurestore feature values served. Resource name: projects/130114571864/locations/us-central1/featurestores/fraud_finder_7551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.featurestore.Featurestore object at 0x7f6ca5c2dd10> \n",
       "resource name: projects/130114571864/locations/us-central1/featurestores/fraud_finder_7551"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_feature_store.batch_serve_to_gcs(\n",
    "    gcs_destination_output_uri_prefix = TRAIN_DATA_URI,\n",
    "    gcs_destination_type = 'csv',\n",
    "    serving_feature_ids = SERVING_FEATURE_IDS, \n",
    "    read_instances_uri = READ_INSTANCES_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:23:59.643261984   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling Uniform bucket-level access for gs://transaction-detective22-1559...\n"
     ]
    }
   ],
   "source": [
    "!gsutil uniformbucketlevelaccess set off gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a copy of the training data in our local notebook instance. We need it later for testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:24:04.760137943   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://transaction-detective22-1559/data/train/000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $TRAIN_DATA_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:24:10.893078716   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://transaction-detective22-1559/data/train/000000000000.csv...\n",
      "/ [1 files][158.2 KiB/158.2 KiB]                                                \n",
      "Operation completed over 1 objects/158.2 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!sudo gsutil cp -r $TRAIN_DATA_URI $TRAIN_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the features into cloud storage, will generate multiple csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = gcs_list(TRAIN_DATA_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Builing a custom fraud detection model\n",
    "\n",
    "### Fixing Imbalanced Dataset\n",
    "In this section, we will deal with imbalance dataset. Specifically, we will randomly delete some of the instances from the non-fraud class in order to match the numbers with the fraud class. This technique is called undersampling. We will skip this step for our workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.parse import urlparse\n",
    "# obj_list = gcs_list(TRAIN_DATA_URI)\n",
    "\n",
    "# TRAIN_DATA_URI_BALANCED = f'{DATA_URI}/balanced_train'\n",
    "# TRAIN_DATA_DIR_BALANCED = os.path.join(DATA_DIR, 'balanced_train')\n",
    "# if not os.path.exists(TRAIN_DATA_DIR_BALANCED):\n",
    "#     os.makedirs(TRAIN_DATA_DIR_BALANCED)\n",
    "# for ds_csv_uri in obj_list:\n",
    "#     tx_df = pd.read_csv(ds_csv_uri)\n",
    "#     shuffled_df = tx_df.sample(frac=1,random_state=4)\n",
    "#     fraud_df = shuffled_df.loc[shuffled_df['tx_fraud'] == 1]\n",
    "#     non_fraud_df = shuffled_df.loc[shuffled_df['tx_fraud'] == 0].sample(n=fraud_df.shape[0],random_state=42)\n",
    "#     balanced_df = pd.concat([fraud_df, non_fraud_df])\n",
    "#     balanced_df.to_csv(os.path.join(TRAIN_DATA_DIR_BALANCED,  os.path.basename(urlparse(ds_csv_uri).path)), index=False)\n",
    "#     break\n",
    "\n",
    "# !gsutil cp -r  $TRAIN_DATA_DIR_BALANCED $TRAIN_DATA_URI_BALANCED\n",
    "# obj_list = gcs_list(TRAIN_DATA_URI_BALANCED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Builing Vertex AI dataset\n",
    "In this section, we will build a Vertex AI dataset from our tabular. Vertex AI datasets can be used to train AutoML models or custom-trained models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/130114571864/locations/us-central1/datasets/4325905354182361088/operations/6720017775349202944\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/130114571864/locations/us-central1/datasets/4325905354182361088\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/130114571864/locations/us-central1/datasets/4325905354182361088')\n",
      "Dataset: sample_train-7551-2022-01-31\n",
      "Name: \t projects/130114571864/locations/us-central1/datasets/4325905354182361088\n"
     ]
    }
   ],
   "source": [
    "dataset = create_gcs_dataset(client=vertex_ai, display_name=DATASET_NAME, gcs_source=obj_list[0]) #obj_list\n",
    "\n",
    "print(\"Dataset:\", f\"{dataset.display_name}\")\n",
    "print(\"Name: \\t\", f\"{dataset.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with custom model\n",
    "In this section, we will use xgboost algorithm. Specifically, we will perform custom training with a pre-built xgboost container.\n",
    "\n",
    "#### Create the training application\n",
    "Typically, to perform custom training you ccan either use a pre-built container, or buid a new container. In this secion we will build a container for xgboost, and use it for training through VertexAI Manged Training serivice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:26:18.380018914   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p -m 777 build_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_training/train_xgb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/train_xgb.py\n",
    "\n",
    "\"\"\"\n",
    "train_gb.py is the module for training a XGBClassifier pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import dask.dataframe as dask_df\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, average_precision_score, f1_score, log_loss, precision_score, recall_score\n",
    "\n",
    "# Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "## Read environmental variables\n",
    "TRAINING_DATA_PATH = os.environ[\"AIP_TRAINING_DATA_URI\"].replace('gs://', '/gcs/')\n",
    "TEST_DATA_PATH = os.environ[\"AIP_TEST_DATA_URI\"].replace('gs://', '/gcs/')\n",
    "MODEL_DIR = os.environ['AIP_MODEL_DIR'].replace('gs://', '/gcs/')\n",
    "MODEL_PATH = MODEL_DIR + 'model.bst'\n",
    "\n",
    "\n",
    "## Training variables\n",
    "LABEL_COLUMN = \"tx_fraud\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_event\",\"terminal_id\",\"customer_id\",\"entity_type_customer\",\"entity_type_terminal\"]\n",
    "DATA_SCHEMA = {\n",
    "\"timestamp\": \"object\",\n",
    "\"entity_type_event\": \"object\",\n",
    "\"tx_amount\": \"float64\",\n",
    "\"customer_id\": \"int64\",\n",
    "\"tx_fraud\": \"int64\",\n",
    "\"terminal_id\": \"int64\",\n",
    "\"entity_type_customer\": \"int64\",\n",
    "\"customer_id_nb_tx_7day_window\": \"int64\",\n",
    "\"customer_id_nb_tx_14day_window\": \"int64\",\n",
    "\"customer_id_nb_tx_1day_window\": \"int64\",\n",
    "\"customer_id_avg_amount_7day_window\": \"float64\",\n",
    "\"customer_id_avg_amount_14day_window\": \"float64\",\n",
    "\"customer_id_avg_amount_1day_window\": \"float64\",\n",
    "\"entity_type_terminal\": \"int64\",\n",
    "\"terminal_id_risk_1day_window\": \"float64\",\n",
    "\"terminal_id_risk_14day_window\": \"float64\",\n",
    "\"terminal_id_risk_7day_window\": \"float64\",\n",
    "\"terminal_id_nb_tx_7day_window\": \"int64\",\n",
    "\"terminal_id_nb_tx_14day_window\": \"int64\",\n",
    "\"terminal_id_nb_tx_1day_window\": \"int64\"\n",
    "}\n",
    "\n",
    "# Helpers -----------------------------------------------------------------------------------------------------------------------------\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data files arguments\n",
    "    parser.add_argument('--bucket', dest='bucket', type=str,\n",
    "                        required=True, help='Bucket uri')\n",
    "    parser.add_argument('--max_depth', dest='max_depth',\n",
    "                        default=6, type=int,\n",
    "                        help='max_depth value.')\n",
    "    parser.add_argument('--eta', dest='eta',\n",
    "                        default=0.4, type=float,\n",
    "                        help='eta.')\n",
    "    parser.add_argument('--gamma', dest='gamma',\n",
    "                        default=0.0, type=float,\n",
    "                        help='eta value')\n",
    "    parser.add_argument(\"-v\", \"--verbose\", \n",
    "                        help=\"increase output verbosity\", \n",
    "                        action=\"store_true\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def set_logging():\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def resample(df, replace, frac=1, random_state = 8):\n",
    "    shuffled_df = df.sample(frac=frac, replace=replace, random_state=random_state)\n",
    "    return shuffled_df\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    numeric_format = {col:\"float32\" for col in numeric_columns}\n",
    "    df.astype(numeric_format)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_model(model, x_true, y_true):\n",
    "    \n",
    "    y_true = y_true.compute()\n",
    "    \n",
    "    #calculate metrics\n",
    "    metrics={}\n",
    "    \n",
    "    y_score =  model.predict_proba(x_true)[:, 1]\n",
    "    y_score = y_score.compute()\n",
    "    fpr, tpr, thr = roc_curve(\n",
    "         y_true=y_true, y_score=y_score, pos_label=True\n",
    "    )\n",
    "    fpr_list = fpr.tolist()[::1000]\n",
    "    tpr_list = tpr.tolist()[::1000]\n",
    "    thr_list = thr.tolist()[::1000]\n",
    "\n",
    "    y_pred = model.predict(x_true)\n",
    "    y_pred.compute()\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    avg_precision_score = round(average_precision_score(y_true, y_score), 3)\n",
    "    f1 = round(f1_score(y_true, y_pred), 3)\n",
    "    lg_loss = round(log_loss(y_true, y_pred), 3)\n",
    "    prec_score = round(precision_score(y_true, y_pred), 3)\n",
    "    rec_score = round(recall_score(y_true, y_pred), 3)\n",
    "    \n",
    "    \n",
    "    metrics['fpr'] = [round(f, 3) for f in fpr_list]\n",
    "    metrics['tpr'] = [round(f, 3) for f in tpr_list]\n",
    "    metrics['thrs'] = [round(f, 3) for f in thr_list]\n",
    "    metrics['confusion_matrix'] = c_matrix.tolist()\n",
    "    metrics['avg_precision_score'] = avg_precision_score\n",
    "    metrics['f1_score'] = f1\n",
    "    metrics['log_loss'] = lg_loss\n",
    "    metrics['precision_score'] = prec_score\n",
    "    metrics['recall_score'] = rec_score\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    if args.verbose:\n",
    "        set_logging()\n",
    "        \n",
    "    #variables\n",
    "    bucket = args.bucket.replace('gs://', '/gcs/')\n",
    "    deliverable_uri = (Path(bucket)/'deliverables')\n",
    "    metrics_uri = (deliverable_uri/'metrics.json')\n",
    "\n",
    "    #read data\n",
    "    train_df = dask_df.read_csv(TRAINING_DATA_PATH, dtype=DATA_SCHEMA)\n",
    "    test_df = dask_df.read_csv(TEST_DATA_PATH, dtype=DATA_SCHEMA)\n",
    "    \n",
    "    #downsampling\n",
    "    train_nfraud_df = train_df[train_df[LABEL_COLUMN]==0]\n",
    "    train_fraud_df = train_df[train_df[LABEL_COLUMN]==1]\n",
    "    train_nfraud_downsample = resample(train_nfraud_df,\n",
    "                          replace=True, \n",
    "                          frac=len(train_fraud_df)/len(train_df))\n",
    "    \n",
    "    downsampled_train_df = dask_df.multi.concat([train_nfraud_downsample, train_fraud_df])\n",
    "    \n",
    "    #preprocessing\n",
    "    preprocessed_train_df = preprocess(downsampled_train_df)\n",
    "    preprocessed_test_df = preprocess(test_df)\n",
    "    \n",
    "    #target, features split\n",
    "    x_train = preprocessed_train_df[preprocessed_train_df.columns.difference([LABEL_COLUMN])]\n",
    "    y_train = preprocessed_train_df.loc[:, LABEL_COLUMN].astype(int)\n",
    "    x_true = preprocessed_test_df[preprocessed_test_df.columns.difference([LABEL_COLUMN])]\n",
    "    y_true = preprocessed_test_df.loc[:, LABEL_COLUMN].astype(int)\n",
    "    \n",
    "    #train model\n",
    "    cluster =  LocalCluster()\n",
    "    client = Client(cluster)\n",
    "    model = xgb.dask.DaskXGBClassifier(objective='reg:logistic', eval_metric='logloss')\n",
    "    model.client = client  # assign the client\n",
    "    model.fit(x_train, y_train, eval_set=[(x_true, y_true)])\n",
    "    if not Path(MODEL_DIR).exists():\n",
    "        Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    model.save_model(MODEL_PATH)\n",
    "    \n",
    "    #generate metrics\n",
    "    metrics = evaluate_model(model, x_true, y_true)\n",
    "    if not Path(deliverable_uri).exists():\n",
    "        Path(deliverable_uri).mkdir(parents=True, exist_ok=True)\n",
    "    with open(metrics_uri, 'w') as file:\n",
    "        json.dump(metrics, file, sort_keys = True, indent = 4)\n",
    "    file.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom image for dask model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:26:58.172367902   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [fraudfinder-7551]\n",
      "Waiting for operation [projects/transaction-detective22-1559/locations/us-centr\n",
      "al1/operations/711a4f11-ac47-42ed-98b8-254e80c1706a] to complete...done.       \n",
      "Created repository [fraudfinder-7551].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:27:10.764017976   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing items under project transaction-detective22-1559, across all locations.\n",
      "\n",
      "                                                                ARTIFACT_REGISTRY\n",
      "REPOSITORY        FORMAT  DESCRIPTION                           LOCATION     LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME\n",
      "fraudfinder-7551  DOCKER  Fraud Finder Docker Image repository  us-central1          Google-managed key  2022-03-14T13:27:07  2022-03-14T13:27:07\n"
     ]
    }
   ],
   "source": [
    "# Create image repo\n",
    "!gcloud artifacts repositories create $IMAGE_REPOSITORY \\\n",
    "    --repository-format=docker \\\n",
    "    --location=us-central1 \\\n",
    "    --description=\"Fraud Finder Docker Image repository\"\n",
    "\n",
    "# List repositories under the project\n",
    "!gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:27:56.549409938   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "Docker configuration file updated.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker us-central1-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/Dockerfile\n",
    "# Specifies base image and tag\n",
    "# FROM us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\n",
    "FROM python:3.7\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install gcsfs numpy pandas scikit-learn dask distributed xgboost --upgrade\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./train_xgb.py /root/train_xgb.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"train_xgb.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:28:10.847847945   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  9.728kB\n",
      "Step 1/5 : FROM python:3.7\n",
      "3.7: Pulling from library/python\n",
      "\n",
      "\u001b[1B1adff207: Pulling fs layer \n",
      "\u001b[1B945c672b: Pulling fs layer \n",
      "\u001b[1B10aec998: Pulling fs layer \n",
      "\u001b[1B8c754e45: Pulling fs layer \n",
      "\u001b[1B762e7602: Pulling fs layer \n",
      "\u001b[1B2e030155: Pulling fs layer \n",
      "\u001b[1Bdc84acc9: Pulling fs layer \n",
      "\u001b[1B4992c7bf: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:b48983bebd0fe1c09639fa008e4cb51aac6277af6c6762fc58ac9d2cb7fc24ef[5A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2KDownloading  170.4MB/196.5MB\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for python:3.7\n",
      " ---> b5b74728b4f4\n",
      "Step 2/5 : WORKDIR /root\n",
      " ---> Running in a55f952aff7d\n",
      "Removing intermediate container a55f952aff7d\n",
      " ---> 068d6d4d5528\n",
      "Step 3/5 : RUN pip install gcsfs numpy pandas scikit-learn dask distributed xgboost --upgrade\n",
      " ---> Running in 9c3a3e8c579e\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2022.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "Collecting dask\n",
      "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting distributed\n",
      "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.2-py3-none-manylinux2014_x86_64.whl (173.6 MB)\n",
      "Collecting aiohttp<4\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.1.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting decorator>4.1.2\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.5.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting fsspec==2022.02.0\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.11.2-py3-none-any.whl (55 kB)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
      "Collecting pyyaml>=5.3.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "Collecting cloudpickle>=1.1.1\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from distributed) (57.5.0)\n",
      "Collecting psutil>=5.0\n",
      "  Downloading psutil-5.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting click>=6.6\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "Collecting tornado>=5\n",
      "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting typing-extensions>=3.7.4\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Collecting locket\n",
      "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting idna>=2.0\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting google-api-core<3.0dev,>=1.29.0\n",
      "  Downloading google_api_core-2.7.1-py3-none-any.whl (114 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
      "  Downloading google_cloud_core-2.2.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=1.3.0\n",
      "  Downloading google_resumable_media-2.3.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.55.0-py2.py3-none-any.whl (212 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: pyasn1, urllib3, six, rsa, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, requests, googleapis-common-protos, google-auth, zipp, typing-extensions, toolz, pyparsing, oauthlib, multidict, locket, google-crc32c, google-api-core, frozenlist, yarl, requests-oauthlib, pyyaml, partd, packaging, numpy, MarkupSafe, importlib-metadata, heapdict, google-resumable-media, google-cloud-core, fsspec, cloudpickle, attrs, asynctest, async-timeout, aiosignal, zict, tornado, threadpoolctl, tblib, sortedcontainers, scipy, pytz, python-dateutil, psutil, msgpack, joblib, jinja2, google-cloud-storage, google-auth-oauthlib, decorator, dask, click, aiohttp, xgboost, scikit-learn, pandas, gcsfs, distributed\n",
      "Successfully installed MarkupSafe-2.1.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.4.0 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 click-8.0.4 cloudpickle-2.0.0 dask-2022.2.0 decorator-5.1.1 distributed-2022.2.0 frozenlist-1.3.0 fsspec-2022.2.0 gcsfs-2022.2.0 google-api-core-2.7.1 google-auth-2.6.0 google-auth-oauthlib-0.5.0 google-cloud-core-2.2.3 google-cloud-storage-2.1.0 google-crc32c-1.3.0 google-resumable-media-2.3.2 googleapis-common-protos-1.55.0 heapdict-1.0.1 idna-3.3 importlib-metadata-4.11.3 jinja2-3.0.3 joblib-1.1.0 locket-0.2.1 msgpack-1.0.3 multidict-6.0.2 numpy-1.21.5 oauthlib-3.2.0 packaging-21.3 pandas-1.3.5 partd-1.2.0 protobuf-3.19.4 psutil-5.9.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.7 python-dateutil-2.8.2 pytz-2021.3 pyyaml-6.0 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 scikit-learn-1.0.2 scipy-1.7.3 six-1.16.0 sortedcontainers-2.4.0 tblib-1.7.0 threadpoolctl-3.1.0 toolz-0.11.2 tornado-6.1 typing-extensions-4.1.1 urllib3-1.26.8 xgboost-1.5.2 yarl-1.7.2 zict-2.1.0 zipp-3.7.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 9c3a3e8c579e\n",
      " ---> 75251e0e26ee\n",
      "Step 4/5 : COPY ./train_xgb.py /root/train_xgb.py\n",
      " ---> 7d2cc20a42eb\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"train_xgb.py\"]\n",
      " ---> Running in f185c5f0c5b5\n",
      "Removing intermediate container f185c5f0c5b5\n",
      " ---> afe398ff0536\n",
      "Successfully built afe398ff0536\n",
      "Successfully tagged us-central1-docker.pkg.dev/transaction-detective22-1559/fraudfinder-7551/dask-xgb-classificator:v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0314 13:29:52.842527906   25880 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/transaction-detective22-1559/fraudfinder-7551/dask-xgb-classificator]\n",
      "\n",
      "\u001b[1B7992c0a5: Preparing \n",
      "\u001b[1B3d4afda0: Preparing \n",
      "\u001b[1Bef05ed8e: Preparing \n",
      "\u001b[1Bde6b4d9c: Preparing \n",
      "\u001b[1Be231faac: Preparing \n",
      "\u001b[1B3ce3bf27: Preparing \n",
      "\u001b[1B3949bffa: Preparing \n",
      "\u001b[1B4a8cee1f: Preparing \n",
      "\u001b[1B4a6f44ae: Preparing \n",
      "\u001b[1B108b2cba: Preparing \n",
      "\u001b[10Bd4afda0: Pushed    1.23GB/1.22GBB\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2Kv1: digest: sha256:5c3a430251cc0f7a34fbc253367c10dba486a316da1126a611d1027dba3017ec size: 2640\n"
     ]
    }
   ],
   "source": [
    "# Build and push docker file\n",
    "!docker build -t $IMAGE_URI ./build_training/\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the script to run on Vertex AI\n",
    "In this section, we create a training pipeline. It will create custom training jobs, load our dataset and upload the model to Vertex AI after the training job is successfully completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://transaction-detective22-1559/aiplatform-custom-training-2022-03-14-13:40:34.334 \n",
      "INFO:google.cloud.aiplatform.training_jobs:No dataset split provided. The service will use a default split.\n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7855333831052623872?project=130114571864\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/130114571864/locations/us-central1/trainingPipelines/7855333831052623872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/130114571864/locations/us-central1/trainingPipelines/7855333831052623872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/130114571864/locations/us-central1/trainingPipelines/7855333831052623872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/130114571864/locations/us-central1/trainingPipelines/7855333831052623872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/130114571864/locations/us-central1/trainingPipelines/7855333831052623872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2754233196869582848?project=130114571864\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob run completed. Resource name: projects/130114571864/locations/us-central1/trainingPipelines/7855333831052623872\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/130114571864/locations/us-central1/models/6524611950385561600\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.CustomContainerTrainingJob(\n",
    "    display_name=TRAIN_JOB_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    "    model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    ")\n",
    "\n",
    "parameters = {\"MAX_DEPTH\": 4, \"ETA\": 0.3, \"GAMMA\": 0.1}\n",
    "\n",
    "CMDARGS = [ f\"\"\"--bucket={BUCKET_NAME}\"\"\",\n",
    "    \"--max_depth=\" + str(parameters[\"MAX_DEPTH\"]),\n",
    "    \"--eta=\" + str(parameters[\"ETA\"]),\n",
    "    \"--gamma=\" + str(parameters[\"GAMMA\"]),\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can run the data through the endpoint, you need to preprocess it to match the format that your custom model defined in task.py expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"tx_fraud\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_event\",\"terminal_id\",\"customer_id\",\"entity_type_customer\",\"entity_type_terminal\"]\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "def preprocess(df):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    dummy_columns = list(df.dtypes[df.dtypes == 'category'].index)\n",
    "    df = pd.get_dummies(df, columns=dummy_columns)\n",
    "\n",
    "    return df\n",
    "#test set\n",
    "train_sample_path = os.path.join(TRAIN_DATA_DIR, '000000000000.csv')\n",
    "df_test = pd.read_csv(train_sample_path)\n",
    "preprocessed_test_Data = preprocess(df_test)\n",
    "\n",
    "x_test = preprocessed_test_Data[preprocessed_test_Data.columns.drop(LABEL_COLUMN).to_list()].values\n",
    "y_test = preprocessed_test_Data.loc[:,LABEL_COLUMN].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we copy the mdoel artifact to the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://transaction-detective22-1559/aiplatform-custom-training-2022-03-14-13:40:34.334/model/model.bst...\n",
      "/ [1 files][ 81.9 KiB/ 81.9 KiB]                                                \n",
      "Operation completed over 1 objects/81.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r $model.uri ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9832954, 0.9832954, 0.9832954, 0.9832954, 0.9832954, 0.9832954,\n",
       "       0.9832954, 0.9832954, 0.9832954, 0.9832954], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "bst = xgb.Booster()  # init model\n",
    "bst.load_model('./model/model.bst') \n",
    "xgtest = xgb.DMatrix(x_test)\n",
    "y_pred_prob = bst.predict(xgtest)\n",
    "y_pred = y_pred_prob.round().astype(int)\n",
    "y_pred_prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5706887328652624, 0.211, 0.07671063395430816, None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test.values, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model\n",
    "Before you use your model to make predictions, you need to deploy it to an Endpoint. You can do this by calling the deploy function on the Model resource. This will do two things:\n",
    "\n",
    "Create an Endpoint resource for deploying the Model resource to.\n",
    "Deploy the Model resource to the Endpoint resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/130114571864/locations/us-central1/endpoints/2144461090535243776/operations/2828907697301094400\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/130114571864/locations/us-central1/endpoints/2144461090535243776\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/130114571864/locations/us-central1/endpoints/2144461090535243776')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/130114571864/locations/us-central1/endpoints/2144461090535243776\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/130114571864/locations/us-central1/endpoints/2144461090535243776/operations/4405167566880768000\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/130114571864/locations/us-central1/endpoints/2144461090535243776\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_COMPUTE='n1-standard-4'\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the deployed model (Make an online prediction request)\n",
    "Send an online prediction request to your deployed model. To make sure your deployed model is working, test it out by sending a request to the endpoint\n",
    "Let's first get test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction input instances need to be formatted as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fraudfinder_xgb_prediction_frmlz-7551-1647254678'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPLOYED_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"instances\": [\n",
    "    [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 3., 0.],\n",
    "    [1., 2., 0., 2., 1., 0., 1., 0., 0., 0., 0., 3., 0.]]\n",
    "  \n",
    "}\n",
    "#In case you want to test it in the console\n",
    "import json\n",
    "with open('predictions.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.09744301438331604, 0.5049852728843689], deployed_model_id='1052531694944387072', explanations=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(instances = payload['instances'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DO NOT RUN) Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "#! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
    "\n",
    "# Delete model resource\n",
    "#! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "#! gsutil -m rm -r $JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
